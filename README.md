# CS 6800 Project - Implementing a RAG model for understanding the relationship between app update frequency and user reviews

# RAG Model with Mistral AI and ChromaDB

This project implements a **Retrieval-Augmented Generation (RAG)** model using **Mistral AI** and **ChromaDB**. The process involves preprocessing app review data, generating embeddings, storing them in ChromaDB, and then retrieving relevant documents to generate a response using Mistral AI.

## **Table of Contents**

1. [Project Setup](#project-setup)
2. [Steps Overview](#steps-overview)
3. [File Descriptions](#file-descriptions)
4. [Running the Project](#running-the-project)
5. [Dependencies](#dependencies)
6. [Environment Variables](#environment-variables)

## **Project Setup**

1. Clone this repository or download the files to your local machine.
2. Make sure you have Python 3.8+ installed.
3. Set up a virtual environment (optional, but recommended):

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scriptsctivate`
   ```

4. Install dependencies by running:
   ```bash
   pip3 install -r requirements.txt
   ```

## **Steps Overview**

The project follows these main steps:

1. **Preprocessing the Data**:
   - Raw JSON files containing app reviews are loaded and preprocessed (i.e., cleaning text).
   - Review content and metadata (like `published_at`) are extracted and combined for embedding generation.
2. **Embedding Generation**:
   - We use the `sentence-transformers` model `all-MiniLM-L6-v2` to create embeddings for the reviews.
   - These embeddings are saved in a JSON file for later use.
3. **Storing Embeddings in ChromaDB**:
   - The generated embeddings are stored in a local ChromaDB instance. ChromaDB is used for efficient vector-based search.
4. **Retrieving Relevant Documents**:
   - Given a query, relevant documents are retrieved from ChromaDB using vector similarity search based on embeddings.
5. **Response Generation Using Mistral AI**:
   - The retrieved documents are passed to the Mistral AI model to generate a relevant response based on the context.

## **File Descriptions**

### **1. preprocess.py**

This script preprocesses the JSON files containing the app reviews, extracts relevant data, and generates embeddings using the `sentence-transformers` model.

**Key Steps**:

- Iterates over JSON files containing app reviews.
- Extracts `published_at` and review content (`google_play_reviews`).
- Cleans the text data by removing special characters and normalizing whitespace.
- Generates embeddings for each review using the `sentence-transformers` model (`all-MiniLM-L6-v2`).
- Saves the embeddings into `embeddings.json`.

### **2. store-database.py**

This script loads the embeddings stored in `embeddings.json` and inserts them into ChromaDB.

**Key Steps**:

- Initializes the ChromaDB client.
- Creates or retrieves a collection named `"app_reviews"`.
- Inserts each embedding along with its metadata (filename, published date, and reviews) into ChromaDB.
- Provides a test query to ensure the embeddings are correctly stored and retrievable.

### **3. retrieval-logic-rag.py**

This script performs the retrieval of relevant documents from ChromaDB and generates a response using Mistral AI.

**Key Steps**:

- Loads the embeddings from ChromaDB based on the query.
- Retrieves the top-k most relevant documents using ChromaDB's query feature.
- Uses the Mistral AI API to generate a response based on the retrieved documents.

### **4. .env**

The `.env` file stores the Mistral API key securely. This file should be placed at the root level of the project.

## **Running the Project**

### **Step 1: Preprocess Data & Generate Embeddings**

To generate embeddings for app reviews, run the `preprocess.py` script:

```bash
python3 preprocess.py
```

This will generate the `embeddings.json` file in the `Embeddings/` directory.

### **Step 2: Store Embeddings in ChromaDB**

After generating the embeddings, run the `store-database.py` script to store them in ChromaDB:

```bash
python3 store-database.py
```

### **Step 3: Retrieve Relevant Documents & Generate Response**

To query the ChromaDB for relevant documents and generate a response using Mistral AI, run the `retrieval-logic.py` script:

```bash
python3 retrieval-logic-rag.py
```

Enter a query when prompted, and the script will return a response generated by Mistral AI based on the context of the retrieved documents.

## **Dependencies**

Here is a list of all the dependencies required for this project. You can install them using the `requirements.txt` file:

- `chromadb>=0.6.0`
- `sentence-transformers>=2.2.0`
- `python-dotenv>=0.19.0`
- `mistralai>=0.1.0`

To install the dependencies:

```bash
pip3 install -r requirements.txt
```

## **Environment Variables**

### **MISTRAL_API_KEY**

The Mistral API key is required to interact with the Mistral AI model. You can store it in a `.env` file in the root directory of the project. Here's an example:

```ini
MISTRAL_API_KEY=your_api_key_here
```

To load the environment variables from this file, the script uses the `python-dotenv` library. Ensure that the `.env` file is placed at the root of your project.

## **Conclusion**

This README should guide you through the process of running and interacting with the RAG model, storing embeddings, and generating responses using Mistral AI. Let me know if you need any further clarifications or adjustments!
